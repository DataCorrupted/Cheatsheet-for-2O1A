\section*{Week 9 (3/1 - 3/7; Appx.B \& Chp.2, Quiz 6)}

\problem{
    f04m2q3
    w08m2q3
    f12f0p2
    w12f0p2
    f13f0p2
    f14f0p2
    f16f0p2
}{
    What is the goal of the memory hierarchy, as stated in class? 
    What two principles make it work? 
    (Two types of the same principle, actually).
}{
    \pg{72} The goal is to provide a memory system with cost per byte almost as low as the cheapest level of memory and speed almost as fast as the fastest level.
    
    \pg{72} \textit{Locality} of reference: \textit{Spacial locality} \& \textit{Temporal locality}.
}


\problem{
    w13f0p1
    w15f0p1
}{
    AMAT
}{
    Average memory access time = Hit time + Miss rate * Miss penalty
}
\problem{
    f15f0p8
    w19f0q37
}{
    Find the Average Memory Access Time (AMAT) for a processor with a 1 ns clock cycle time, a miss penalty of 20 clock cycles, a miss rate of 0.10 misses per instruction, and a cache access time (including hit detection) of 1 clock cycle. 
    Assume that the read and write miss penalties are the same and ignore other write stalls.
}{
    \begin{align*}
        \texttt{AMAT} & = \texttt{Hit time} + \texttt{Miss rate} * \texttt{Miss penalty} \\
                      & = 1 \texttt{ clock} * 1 \texttt{ ns/clock} + 0.10 * 20 \texttt{ clock(s)} * 1 \texttt{ ns/clock}  \\
                      & = 3 \texttt{ ns}
    \end{align*}
}

\problem{
    f12m2q5
    f13m2q3
}{
    Roofline plots what vs what? 
    Why is this information important?
}{
    Arithmetic intensity vs float point performance \& memory performance.

    It lets you calculate if you need to work on improving the memory bandwidth or the float-point hardware.
}

\subsection*{Cache}

\problem{
    w19f0q3
}{
    Does an average program's locality behavior stay the same while it is executing, or does it vary?
}{
    Vary
}

\problem{
}{
    What is \textit{2:1 cache rule of thumb}?
}{
    A directed mapped cache of size N has about the same miss rate as a two-way set associative cache of size N/2.
}

\problem{
    f13f0p3
    f16m2q1
    w19m2q3
}{
    Writes to a cache are inherently slower than reads from a cache. Why?
}{
    You can read the tag and simultaneously read the data location, Can discard the data read if the tag didn't match.
    However, you cannot do that on a write, because the write changes the state of the machine - you must first do the tag compare to ensure the write should, in fact change the state.

    OR

    Read can do check tags and read data in parallel. 
    However, writing is inherently serial/sequential, so we have to check tags before writing the data to the cache. 

}

\problem{
    w13f0p1
    w15f0p1
    w19f0q5
}{
    Which type of cache miss can be reduced by using shorter lines?
}{
    Capacity
}
\problem{
    w13f0p1
    w15f0p1
    w19f0q6
}{
    Which type of cache miss can be reduced by using longer lines?
}{
    Compulsive
}
\problem{
    w13f0p1
    w15f0p1
    w19f0q7
}{
    Using a different mapping scheme will reduce which type of cache miss?
}{
    Conflict
}

\problem{
    w19f0q34
}{
    The designer has the choice of using a physically addressed cache or a virtually addressed cache.
    Explain the difference (drawing a picture is fine!), and give 1 advantage for each.
}{
    \warn{
        The virtually addressed cache is closer to CPU and doesn't need TLB to be addressed.
        Yet physically addressed cache has to be translated by TLB before it can be indexed.

        VA: Fasten hit time.
        PA: Don't need to squash cache on context switch.
    }
}

\problem{
}{
    Why we don't use LRU? 
    What are we using instead?
}{
    \pg{B-9}
    
    As the number of blocks to keep track of increases, LRU becomes increasingly expensive
    
    FIFO
}


\problem{
    f13f0p8
    f15f0p7
    f16m2q16
    w19f0q52
}{
    Assuming a 19-bit address and a 256-byte Direct Mapped cache with a linesize=2, show how an address is partitioned/interpreted by the cache.
}{
    \begin{align*}
        & \texttt{offset} = 1 \texttt{bit }   \\
        & 256/2 = 128 =  2^7                  \\
        & \texttt{entry} = 7 \texttt{bits}    \\
        & \texttt{tag}  = 11\texttt{ bits}
    \end{align*}
}

\problem{
    f13f0p8
    f15f0p7
    f16m2q17
    w19f0q53
}{
    Assuming a 19-bit address and an 80-byte 10-way Set Associative cache with a linesize=4, show how an address is partitioned/interpreted by the cache.
}{
    \begin{align*}
        & \texttt{offset} = 2 \texttt{bits}   \\
        & 80/(10*4) = 2 = 2^1                 \\
        & \texttt{  set} = 1 \texttt{bit }    \\
        & \texttt{tag} = 16 \texttt{bits}
    \end{align*}
}

\problem{
    f13f0p8
    f15f0p7
    f16m2q18
    w19f0q54
}{
    Assuming a 19-bit address and a 328-byte Fully Associative cache with a linesize=8, show how an address is partitioned/interpreted by the cache.
}{
    \begin{align*}
        & \texttt{offset} =  3 \texttt{bits} \\
        & \texttt{tag   } = 16 \texttt{bits}        
    \end{align*}
}

\problem{
    f13f0q6
    f15f0p5
    w19f0q38
}{
    For each technique in the following table, indicate how(on average) the 3 terms of AMAT are affected.
    (Assume there is a single L1 cache.)
}{
    \begin{center}
        \begin{tabular}{  m{0.35\textwidth} C{0.15\textwidth} C{0.15\textwidth} C{0.15\textwidth} }
        \toprule
            Techinque                     & Hit time & Miss rate & Miss penalty \\
        \midrule
            Increasing line/block size    &   & - & + \\ \hline
            Increasing Associativity      & + & - &   \\ \hline
            Decreasing cache size         & - & + &   \\ \hline     
            HW/compiler controlled prefetch
                                          &   & - & - \\ \hline 
            Compiler optimizations        &   & - &   \\ \hline 
            Non-blocking cache            &   &   & - \\ \hline  
            Virtually addressed cache     & - &   &   \\ \hline       
            Victim cache(victim cache and regular cache accessed in parallel)
                                          &   & - &   \\ \hline    
            Critical word first           &   &   & - \\ \hline        
            Multilevel cache(first level) & - & + & - \\ \hline
            Give priority to read translation during indexing of a cache
                                          &   &   & - \\ \hline      
            Merging write bufer           &   &   & - \\ \hline        
            Small \& simple first-level cache 
                                          & - &   &   \\ \hline
            Way prediction                & - &   &   \\ 
        \bottomrule
        \end{tabular}
    \end{center}
}

\subsection*{Memory}


\problem{
    f13f0p8
    f15f0p7
    f16m2q19
    w19f0q55
}{
    Given a 1 Megabyte physical memory, a 22-bit Virtual address, and a page size of 1K bytes, write down the number of entries in the Page Table, and the width of each entry.
}{
    \begin{align*}
        & 1M = 2^{20}; 1K = 2^{10} \\
        & (2^{20})/(2^{10}) = 2^{10} \rightarrow \texttt{10-bit wide }
    \end{align*}
    PM: $ 10 | 10 $ \\
    VM: $ 12 | 10 $ \\
    $2^{12}$ \texttt{entries}        
}

\problem{
    f13f0p8
    f15f0p7
    f16m2q20
    w19f0q56
}{
    Given a 1 Megabyte physical memory, a 34-bit Virtual address, and a page size of 2K bytes, write down the number of entries in the Page Table, and the width of each entry. Is there a problem with this configuration? 
    If so, what is the problem?
}{
    \begin{align*}
        & 1M = 2^{20}; 2K = 2^{11} \\
        & (2^{20})/(2^{11}) = 2^9 \rightarrow \texttt{ 9-bit wide }
    \end{align*}
    PM: $  9 | 11 $ \\
    VM: $ 23 | 11 $ \\
    $2^{23}$ \texttt{entries}         \\
    Problem: It is too large to store in 1M physical memory. We can use two page tables. One is base, another one is secondary.
}